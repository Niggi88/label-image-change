# review_api_batch.py
from fastapi import FastAPI, HTTPException, Query
from fastapi.staticfiles import StaticFiles
from pathlib import Path
from datetime import datetime
import subprocess
import json
from typing import Any, Dict, Iterable, List, Optional
import uuid
from collections import Counter
from validate_uploads import validate_results_payload
import logging
from loguru import logger



_missing_counter = Counter()

app = FastAPI(title="Unsure Review API (read-only)")

# --- Hardcoded paths on ml01 ---
CHANGE_ROOT = Path("/opt/datasets/change_detection/change_data").resolve()
IMAGES_DIR  = CHANGE_ROOT / "images"
USER_DIRS   = [CHANGE_ROOT / "sarah", CHANGE_ROOT / "niklas", CHANGE_ROOT / "santiago", CHANGE_ROOT / "almas"]

# Generated by the extractor script
INCONSISTENT_PATH = CHANGE_ROOT / "review_batches"

# Where batches are persisted
BATCH_DIR = CHANGE_ROOT / "review_batches"
BATCH_DIR.mkdir(parents=True, exist_ok=True)

MODELS_DIR = Path("/opt/software/change_detection/models")
REVIEW_BATCH_DIR = Path("/opt/datasets/change_detection/change_data/review_batches")

# Where the on-demand extractor lives
EXTRACTOR = Path("/opt/software/change_detection/cart_dataScience_snapshotChangeModel/server_scripts/extract_false_labeling_server.py")

from pathlib import Path

MODELS_DIR = Path("/opt/software/change_detection/models")
REVIEW_BATCH_DIR = Path("/opt/datasets/change_detection/change_data/review_batches")

@app.get("/api/inconsistent/models")
def list_available_models():

    models = []

    if not MODELS_DIR.exists() or not REVIEW_BATCH_DIR.exists():
        return models

    # iterate over extractor jsons: <name>.json
    for batch_folder in REVIEW_BATCH_DIR.iterdir(): #?
        for batch_file in batch_folder.glob("*.json"):
            base_name = batch_file.stem          # "<name>"
            model_file = MODELS_DIR / f"{base_name}.pth"

            if model_file.exists() and model_file.is_file():
                models.append({
                    "modelName": base_name,
                })

    models.sort(key=lambda x: x["modelName"])
    return models



# Serve image files at /images/<relative_path>
app.mount("/images", StaticFiles(directory=str(IMAGES_DIR)), name="images")

def _iter_annotation_files():
    """Yield (user, file Path) for all *.json inside each user dir."""
    for user_dir in USER_DIRS:
        if not user_dir.exists():
            continue
        user = user_dir.name
        for jf in sorted(user_dir.glob("*.json")):
            yield user, jf

def _sorted_unsure_unassigned(exclude_user) -> List[Dict[str, Any]]:
    assigned = _assigned_keys()
    items: List[Dict[str, Any]] = []
    for rec in list_unsure_pairs(limit=999999):  # reuse your existing code
        k = f"{rec['store_session_path']}|{int(rec['pair_id'])}"
        if k in assigned:
            continue
        if exclude_user and rec.get("unsure_by", {}).get("name") == exclude_user:
            continue 
        items.append(rec)
    # sort if needed (e.g. FIFO by pair_id or no sorting)
    def _ts(x):
        try:
            return datetime.fromisoformat(x.get("timestamp") or "")
        except Exception:
            return datetime.min
    items.sort(key=_ts, reverse=False)
    return items

@app.get("/unsure/batch")
def get_or_create_unsure_batch(user: str, size: int = 50):
    if not user:
        raise HTTPException(400, "user is required")

    active = _find_active_batch_for_user(user, batch_type="unsure")
    if active:
        return active

    pool = _sorted_unsure_unassigned(exclude_user=user)
    if not pool:
        return {"message": "no unassigned unsure items left", "items": [], "count": 0}

    batch_items = pool[:max(1, int(size))]
    batch_id = datetime.now().strftime("%Y%m%d_%H%M%S_") + uuid.uuid4().hex[:6]
    payload = {
        "batch_id": batch_id,
        "batch_type": "unsure",
        "created": datetime.now().isoformat(),
        "status": "assigned",
        "reviewer": user,
        "size_requested": int(size),
        "count": len(batch_items),
        "items": batch_items,
        "results": {},
    }
    _write_json_atomic(_batch_path(batch_id), payload)
    return payload


def list_unsure_pairs(limit: int = 999999):
    global _missing_counter
    _missing_counter = Counter()

    out = []
    for user, jf in _iter_annotation_files():
        try:
            data = json.loads(jf.read_text())
        except Exception as e:
            print("[UNSURE] skip", jf, "->", e)
            continue

        for k, entry in data.items():
            if k == "_meta":
                continue
            ps = entry.get("pair_state")
            if ps not in (None, "no_annotation"):
                continue

            raw1 = entry.get("im1_path")
            raw2 = entry.get("im2_path")
            if not raw1 or not raw2:
                continue

            p1 = (IMAGES_DIR / raw1).resolve()
            p2 = (IMAGES_DIR / raw2).resolve()
            if not (p1.exists() and p2.exists()):
                print(f"[UnsureBatch] Missing: {p1} or {p2} (user={user})")
                _missing_counter[user] += 1
                continue

            # ðŸ”‘ Normalize session/store path from the relative image path
            store_session_path = str(Path(raw1).parent.parent.as_posix())
            session_id = store_session_path   # keep full store/session combo

            file_ts = datetime.fromtimestamp(jf.stat().st_mtime).isoformat()

            out.append({
                "session_id": session_id,
                "store_session_path": store_session_path,
                "pair_id": int(k),
                "im1_name": Path(raw1).name,
                "im2_name": Path(raw2).name,
                "im1_url": _image_url(raw1),
                "im2_url": _image_url(raw2),
                "unsure_by": {"name": user},
                "timestamp": file_ts,
            })

            if len(out) >= limit:
                return out

    if _missing_counter:
        print("[UnsureBatch] Missing images summary:")
        for usr, cnt in _missing_counter.items():
            print(f"  {usr}: {cnt} missing images")

    return out


def _records_from_inconsistent(path: Path) -> Iterable[Dict[str, Any]]:
    """
    Reads inconsistent_reviews.json which our extractor writes as a dict {key: record}.
    Supports list fallback just in case.
    """
    if not path.exists():
        return []
    try:
        raw = json.loads(path.read_text())
    except Exception as e:
        print("[INCONSISTENT] read error:", e)
        return []
    if isinstance(raw, dict):
        return raw.values()
    if isinstance(raw, list):
        return raw
    return []


def _image_url(rel_path: Optional[str]) -> Optional[str]:
    if not rel_path:
        return None
    # ensure it's a relative path under /images (avoid double '/images/images/...'):
    rel = rel_path.lstrip("/")
    if rel.startswith("images/"):
        rel = rel[len("images/"):]
    return f"/images/{rel}"




# ------------------- BATCH QUEUE ADDITIONS -------------------

def _batch_path(batch_id: str) -> Path:
    return BATCH_DIR / f"review_batch_{batch_id}.json"


def _load_batch(batch_id: str) -> Dict[str, Any]:
    p = _batch_path(batch_id)
    if not p.exists():
        raise FileNotFoundError(batch_id)
    return json.loads(p.read_text())


def _write_json_atomic(path: Path, payload: Dict[str, Any]) -> None:
    tmp = path.with_suffix(".tmp")
    tmp.write_text(json.dumps(payload, indent=2))
    tmp.replace(path)


def _all_batches() -> List[Dict[str, Any]]:
    out = []
    for jf in sorted(BATCH_DIR.glob("review_batch_*.json")):
        try:
            data = json.loads(jf.read_text())
            data["_path"] = str(jf)
            out.append(data)
        except Exception as e:
            print("[BATCH] skip", jf, "->", e)
    return out


def _assigned_keys() -> set:
    """Collect keys already assigned to any batch to avoid duplicates across users."""
    keys = set()
    for b in _all_batches():
        for it in b.get("items", []):
            # key = store_session_path|pair_id (mirrors extractor upsert key)
            k = f"{it['store_session_path']}|{int(it['pair_id'])}"
            keys.add(k)
    return keys


def _sorted_inconsistent_unassigned(selected_users, selected_model) -> List[Dict[str, Any]]:
    assigned = _assigned_keys()
    items: List[Dict[str, Any]] = []
    for rec in _records_from_inconsistent(INCONSISTENT_PATH / f"batches_{selected_model}" / f"{selected_model}.json"):
        raw1 = rec.get("im1_path") or f"{rec['store_session_path']}/{rec['im1_name']}"
        raw2 = rec.get("im2_path") or f"{rec['store_session_path']}/{rec['im2_name']}"
        if not raw1 or not raw2:
            continue

        p1 = (IMAGES_DIR / raw1).resolve()
        p2 = (IMAGES_DIR / raw2).resolve()
        if not (p1.exists() and p2.exists()):
            print(f"[InconsistentBatch] Missing: {p1} or {p2}")
            continue

        # Normalize session/store path from relative image path
        store_session_path = rec.get("store_session_path")
        # session_id = store_session_path

        k = f"{store_session_path}|{int(rec.get('pair_id', -1))}"

        if k in assigned:
            logger.info(f"{selected_users} already assigned")
            continue
        annotator = (
            rec.get("annotated_by", {})
            or rec.get("unsure_by", {})
            or rec.get("reviewed_by", {})
        )

        if selected_users and annotator not in selected_users:
            continue



        items.append({
            # "session_id": session_id,
            "timestampExtractor": rec.get("timestampExtractor"),
            "timestampOriginalAnnotation": rec.get("timestampOriginalAnnotation"),
            "store_session_path": store_session_path,
            "pair_id": int(rec.get("pair_id", -1)),
            "im1_url": _image_url(raw1),
            "im2_url": _image_url(raw2),
            "predicted": rec.get("predicted"),
            "expected": rec.get("expected"),
            "annotated_by": rec.get("annotated_by") or {},
            "boxes_expected": rec.get("boxes_expected", []),
            "boxes_predicted": rec.get("boxes_predicted", []),
            "image1_size": rec.get("image1_size"),
            "image2_size": rec.get("image2_size"),
            "model_name": rec.get("model_name"),
            "confidence": rec.get("confidence"),
        })
        logger.info(f"new batch created: {items}")
        # logger.info("items:")
        # logger.info(items)
    # FIFO -> oldest first
    def _ts(x):
        try:
            return datetime.fromisoformat(x.get("timestamp") or "")
        except Exception:
            return datetime.min
    items.sort(key=_ts, reverse=False)
    logger.info(f"new batch created: {items}")
    return items


def _find_active_batch_for_user(user: str, batch_type: Optional[str] = None) -> Optional[Dict[str, Any]]:
    for b in _all_batches():
        if b.get("reviewer") != user:
            continue
        if batch_type and b.get("batch_type") != batch_type:
            continue
        # check if result already exists
        results_path = _results_path(b["batch_id"])
        if results_path.exists():
            continue  # batch already finished
        if b.get("status") in ("assigned", "in_progress"):
            logger.info(f"got active batch: {b}")
            return b
    return None




@app.get("/inconsistent/batch")
def get_or_create_inconsistent_batch(user: str, size: int = 5, selected_users: list[str] = Query(default=[]), selected_model: str = None):
    """
    Reserve a unique batch for a user (queue). If the user already has an active batch,
    return it. Otherwise create a new batch of up to `size` items (FIFO).
    """
    if not user:
        raise HTTPException(400, "user is required")

    # reuse existing active batch
    active = _find_active_batch_for_user(user, batch_type="inconsistent")
    if active:
        logger.info("takes active batch")
        return active

    # create a new batch
    pool = _sorted_inconsistent_unassigned(selected_users=selected_users, selected_model=selected_model)

    logger.info("getting from pool")
    if not pool:
        return {"message": "no unassigned items left", "items": [], "count": 0}

    batch_items = pool[:max(1, int(size))]
    batch_id = datetime.now().strftime("%Y%m%d_%H%M%S_") + uuid.uuid4().hex[:6]
    payload = {
        "batch_id": batch_id,
        "batch_type": "inconsistent",
        "created": datetime.now().isoformat(),
        "status": "assigned",
        "reviewer": user,
        "size_requested": int(size),
        "count": len(batch_items),
        "items": batch_items,
	    "model_name": batch_items[0].get("model_name") if batch_items else None,
        "confidence": batch_items[0].get("confidence") if batch_items else None,
        "results": {},  # client will POST corrections here
    }
    _write_json_atomic(_batch_path(batch_id), payload)
    return payload


def _results_path(batch_id: str) -> Path:
    batch = _load_batch(batch_id)

    model_name = batch.get("model_name")

    base = Path("/opt/datasets/change_detection/change_data/review_batches")

    if not model_name:
        raise ValueError(f"Batch {batch_id} has no model_name")


    return (
            base
            / f"batches_{model_name}"
            / f"results_{model_name}"
            / f"review_batch_{batch_id}.json"
        )





# logger = logging.getLogger("review_api")

@app.post("/batches/{batch_id}/results")
def upload_batch_results(batch_id: str, results: Dict[str, Any]):
    """
    Save corrected annotations for a batch into *_results folder.
    `results` is a dict keyed by 'store_session_path|pair_id' -> corrected record.
    """

    try:
        batch = _load_batch(batch_id)
        validate_results_payload(batch, results)
    except FileNotFoundError:
        logger.warning(f"[RESULT UPLOAD] batch not found: {batch_id}")
        raise HTTPException(status_code=404, detail=f"batch {batch_id} not found")

    except AssertionError as e:
        logger.warning(f"[VALIDATION FAILED] batch={batch_id} error={e}")
        raise HTTPException(status_code=422, detail=str(e))



    status = (results.get("_meta") or {}).get("status", "uploaded")

    out_path = _results_path(batch_id)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    _write_json_atomic(out_path, results)


    return {"ok": True, "status": status, "count_results": len(results)}
    


# return random image pairs for validation


import random

def iter_changed_review_pairs():
    """
    Iterate over all reviewed result files and yield pairs
    where previously != reviewed.
    """
    results_dir = CHANGE_ROOT / "review_batches" / "inconsistent_results"
    
    for jf in results_dir.glob("*.json"):
        try:
            data = json.loads(jf.read_text())
        except Exception:
            continue

        items = data.get("items", data)

        for key, rec in items.items():
            if key == "_meta":
                continue

            prev = rec.get("previously")
            if not prev:
                continue

            reviewed_state = rec.get("pair_state")
            reviewed_boxes = rec.get("boxes")

            # if reviewed_state is None or reviewed_boxes is None:
            #     continue

            changed = (
                reviewed_state != prev.get("pair_state")
                or reviewed_boxes != prev.get("boxes")
            )

            if not changed:
                continue

            yield {
                "key": key,
                "im1_url": _image_url(rec.get("im1_path")),
                "im2_url": _image_url(rec.get("im2_path")),
                "image1_size": rec.get("image1_size"),
                "image2_size": rec.get("image2_size"),
                "previously": {
                    "pair_state": prev.get("pair_state"),
                    "boxes": prev.get("boxes"),
                    "annotator": prev.get("annotator"),
                    "reviewer": prev.get("reviewer"),
                    "timestamp": prev.get("timestampOriginalAnnotation"),
                },
                "reviewed": {
                    "batch_timestamp":  data.get("_meta", {}).get("timestamp"),
                    "pair_state": reviewed_state,
                    "boxes": reviewed_boxes,
                },
            }

def sample_changed_review_pairs(limit: int = 10):
    all_items = list(iter_changed_review_pairs())
    if not all_items:
        return []
    return random.sample(all_items, min(limit, len(all_items)))


from datetime import datetime, timedelta



@app.get("/review/changed/random")
def get_random_changed_reviews(limit: int = 10):
    """
    Return random image pairs where previously != reviewed.
    Read-only inspection endpoint.
    """
    items = sample_changed_review_pairs(limit=limit)

    return {
        "count": len(items),
        "items": items,
    }

def is_batch_from_yesterday(batch_data) -> bool:
    ts = batch_data.get("_meta", {}).get("timestamp")
    if not ts:
        return False
    try:
        t = datetime.fromisoformat(ts)
    except Exception:
        return False

    yesterday = datetime.now().date() - timedelta(days=1)
    return t.date() == yesterday


def iter_changed_review_pairs_yesterday():
    results_dir = CHANGE_ROOT / "review_batches" / "inconsistent_results"

    for jf in results_dir.glob("*.json"):
        try:
            data = json.loads(jf.read_text())
        except Exception:
            continue

        if not is_batch_from_yesterday(data):
            continue

        items = data.get("items", data)

        for key, rec in items.items():
            if key == "_meta":                
                continue

            prev = rec.get("previously")
            if not prev:
                continue

            reviewed_state = rec.get("pair_state")
            reviewed_boxes = rec.get("boxes")

            changed = (
                reviewed_state != prev.get("pair_state")
                or reviewed_boxes != prev.get("boxes")
            )

            # if not changed:
            #     continue

            yield {
                "key": pair_to_id_string_from_entry(rec),
                "batch": jf.stem,
                "im1_url": _image_url(rec.get("im1_path")),
                "im2_url": _image_url(rec.get("im2_path")),
                "image1_size": rec.get("image1_size"),
                "image2_size": rec.get("image2_size"),
                "file_path": str(jf),
                "previously": {
                    "pair_state": prev.get("pair_state"),
                    "boxes": prev.get("boxes"),
                    "annotator": prev.get("annotator"),
                    "reviewer": prev.get("reviewer"),
                    "timestamp": prev.get("timestampOriginalAnnotation"),
                },
                "reviewed": {
                    "timestamp": data.get("_meta", {}).get("timestamp"),
                    "pair_state": reviewed_state,
                    "boxes": reviewed_boxes,
                },
            }


@app.get("/review/changed/yesterday")
def get_changed_reviews_yesterday():
    items = list(iter_changed_review_pairs_yesterday())

    if not items:
        return {
            "count": 0,
            "items": [],
        }

    return {
        "count": len(items),
        "items": items,
    }


###############
def iter_all_review_pairs_with_issues():
    results_dir = CHANGE_ROOT / "review_batches" / "inconsistent_results"

    for jf in results_dir.glob("*.json"):
        try:
            data = json.loads(jf.read_text())
        except Exception:
            continue

        batch_timestamp = data.get("_meta", {}).get("timestamp")
        items = data.get("items", {})

        for key, rec in items.items():
            prev = rec.get("previously")
            if not prev:
                continue

            pair = {
                "key": key,
                "batch": jf.stem,
                "im1_url": _image_url(rec.get("im1_path")),
                "im2_url": _image_url(rec.get("im2_path")),
                "image1_size": rec.get("image1_size"),
                "image2_size": rec.get("image2_size"),
                "previously": {
                    "pair_state": prev.get("pair_state"),
                    "boxes": prev.get("boxes"),
                    "annotator": prev.get("annotator"),
                    "reviewer": prev.get("reviewer"),
                    "timestamp": prev.get("timestampOriginalAnnotation"),
                },
                "reviewed": {
                    "batch_timestamp": batch_timestamp,
                    "pair_state": rec.get("pair_state"),
                    "boxes": rec.get("boxes"),
                },
            }

            issues = classify_known_issues(pair)
            yield pair, issues



def classify_known_issues(pair):
    issues = []

    reviewed = pair["reviewed"]
    prev = pair["previously"]

    state = reviewed.get("pair_state")
    boxes = reviewed.get("boxes") or []
    expected = prev.get("pair_state")
    expected_boxes = prev.get("boxes") or []

    # Fehler 1
    if state is None:
        issues.append("NONE_STATE_AFTER_RESET")

    # Fehler 2
    if state == "added" and not boxes:
        issues.append("ADDED_WITHOUT_BOXES")


    if state == "annotated" and not boxes:
        issues.append("ANNOTATED_WITHOUT_BOXES")

    return issues


from collections import Counter

def collect_issue_stats(pairs):
    stats = Counter()
    bad_pairs = []

    for p in pairs:
        issues = classify_known_issues(p)
        if issues:
            bad_pairs.append((p, issues))
            for i in issues:
                stats[i] += 1

    return stats, bad_pairs


from pathlib import Path

def pair_to_id_string(pair: dict) -> str:
    """
    store_id__session_id__image1__image2
    """
    p1 = Path(pair["im1_url"])
    p2 = Path(pair["im2_url"])

    # .../store_x/session_y/image.jpeg
    store_id = p1.parts[-3]
    session_id = p1.parts[-2]
    image1 = p1.stem
    image2 = p2.stem

    return f"{store_id}__{session_id}__{image1}__{image2}"


def is_added_without_boxes(pair: dict) -> bool:
    return (
        pair["previously"].get("pair_state") == "added"
        and pair["reviewed"].get("pair_state") == "added"
        and not (pair["reviewed"].get("boxes") or [])
    )

@app.get("/review/validate/known_issues")
def validate_known_issues(limit: int = 20):
    stats = Counter()
    examples = []
    total = 0
    bad = 0
    added_without_boxes_ids = []

    for pair, issues in iter_all_review_pairs_with_issues():
        total += 1

        if issues:
            bad += 1
            for i in issues:
                stats[i] += 1

            if len(examples) < limit:
                examples.append(pair)

        if is_added_without_boxes(pair):
            added_without_boxes_ids.append(pair_to_id_string(pair))

    return {
        "summary": {
            "NONE_STATE_AFTER_RESET": stats.get("NONE_STATE_AFTER_RESET", 0),
            "ADDED_WITHOUT_BOXES": stats.get("ADDED_WITHOUT_BOXES", 0),
            "ANNOTATED_WITHOUT_BOXES": stats.get("ANNOTATED_WITHOUT_BOXES", 0),
        },
        "added_without_boxes": added_without_boxes_ids,
        "total_pairs": total,
        "pairs_with_issues": bad,
        "examples": examples,
    }


#########################

import json
from pathlib import Path

CHANGE_ROOT = Path("/opt/datasets/change_detection/change_data")
USERS = ["almas", "niklas", "santiago", "sarah"]

def iter_all_user_pairs():
    for user in USERS:
        user_root = CHANGE_ROOT / user
        if not user_root.exists():
            continue

        for jf in user_root.glob("**/*.json"):
            try:
                data = json.loads(jf.read_text())
            except Exception:
                continue

            meta = data.get("_meta", {})
            for item_id, entry in data.items():
                if item_id == "_meta":
                    continue

                yield {
                    "user": user,
                    "file": str(jf),
                    "item_id": item_id,
                    "pair": entry,
                    "meta": meta,
                }

def classify_user_pair_issues(entry: dict):
    issues = []

    state = entry.get("pair_state")
    boxes = entry.get("boxes") or []
    prev = entry.get("previously")

    # 1. NONE state
    if state is None:
        issues.append("NONE_STATE")

    # 2. added without boxes
    if state == "added" and not boxes:
        issues.append("ADDED_WITHOUT_BOXES")

    # 3. annotated without boxes
    if state == "annotated" and not boxes:
        issues.append("ANNOTATED_WITHOUT_BOXES")

    # 4. boxes where not allowed
    if state in ("nothing", "chaos", "no_annotation") and boxes:
        issues.append("BOXES_WHERE_NOT_ALLOWED")

    if state in ("item_added"):
        issues.append("ITEM_ADDED_AS_PAIR_STATE")

    return issues


def build_client_pair(entry: dict, meta: dict):
    # ORIGINAL: immer vorhanden
    original = {
        "pair_state": entry.get("pair_state"),
        "boxes": entry.get("boxes", []),
        "timestamp": meta.get("timestamp"),
        "annotator": meta.get("user"),
    }

    previously = None
    reviewed = None

    if "previously" in entry:
        previously = entry["previously"]

        reviewed = {
            "pair_state": entry.get("pair_state"),
            "boxes": entry.get("boxes", []),
            "timestamp": entry.get("timestamp_reviewed"),
            "reviewer": previously.get("reviewer"),
        }

    return original, previously, reviewed



from pathlib import Path

def pair_to_id_string_from_entry(entry: dict) -> str:
    """
    store_id__session_id__image1__image2
    """
    p1 = Path(entry["im1_path"])
    p2 = Path(entry["im2_path"])

    store_id = p1.parts[-3]
    session_id = p1.parts[-2]

    return f"{store_id}__{session_id}__{p1.stem}__{p2.stem}"

from collections import Counter

def validate_user_change_data(limit=20):
    stats = Counter()
    examples = []
    bad_ids = []

    total = 0
    bad = 0

    for rec in iter_all_user_pairs():
        total += 1
        entry = rec["pair"]

        issues = classify_user_pair_issues(entry)
        if not issues:
            continue

        bad += 1
        for i in issues:
            stats[i] += 1

        pair_id = pair_to_id_string_from_entry(entry)
        bad_ids.append(pair_id)

        if len(examples) < limit:
            original, previously, reviewed = build_client_pair(entry, rec["meta"])

            examples.append({
                "key": pair_id,
                "user": rec["user"],
                "file": rec["file"],
                "item_id": rec["item_id"],

                "im1_url": _image_url(entry.get("im1_path")),
                "im2_url": _image_url(entry.get("im2_path")),

                "image1_size": entry.get("image1_size"),
                "image2_size": entry.get("image2_size"),

                "original": original,
                "previously": previously,
                "reviewed": reviewed,

                "issues": issues,
            })


    return {
        "summary": dict(stats),
        "total_pairs": total,
        "pairs_with_issues": bad,
        "bad_pair_ids": bad_ids,
        "examples": examples,
    }

@app.get("/validate/change_data/known_issues")
def validate_change_data_known_issues(limit: int = 20):
    return validate_user_change_data(limit=limit)

##############

import random

def iter_all_user_pairs_normalized():
    for rec in iter_all_user_pairs():
        entry = rec["pair"]
        meta = rec["meta"]

        original, previously, reviewed = build_client_pair(entry, meta)

        yield {
            "key": pair_to_id_string_from_entry(entry),
            "user": rec["user"],
            "file_path": rec["file"],
            "im1_url": _image_url(entry.get("im1_path")),
            "im2_url": _image_url(entry.get("im2_path")),

            "image1_size": entry.get("image1_size"),
            "image2_size": entry.get("image2_size"),

            "original": original,
            "previously": previously,
            "reviewed": reviewed,

            "issues": classify_user_pair_issues(entry),
        }

@app.get("/change_data/random")
def get_random_change_data_pairs(
    limit: int = 10,
    only_with_issues: bool = False,
):
    all_pairs = list(iter_all_user_pairs_normalized())

    if only_with_issues:
        all_pairs = [p for p in all_pairs if p["issues"]]

    if not all_pairs:
        return {"count": 0, "items": []}

    items = random.sample(all_pairs, min(limit, len(all_pairs)))

    return {
        "count": len(items),
        "items": items,
    }

@app.get("/change_data/recent")
def get_recent_change_data_pairs(
    limit: int = 10,
    recently_until: int = 2,
    annotator : Optional[str] = None,
    reviewer : Optional[str] = None,
    sorted: bool = True,
):
    all_pairs = list(iter_all_user_pairs_normalized())

    def _get_sort_timestamp(pair):
        if pair["reviewed"] and pair["reviewed"].get("timestamp"):
            return datetime.fromisoformat(pair["reviewed"]["timestamp"])
        if pair["original"] and pair["original"].get("timestamp"):
            return datetime.fromisoformat(pair["original"]["timestamp"])
        return datetime.min
    
    if sorted:
        all_pairs.sort(
            key=_get_sort_timestamp,
            reverse=True
        )


    recently = datetime.now() - timedelta(days=recently_until)
    def _is_recent(pair):
        try:
            ts = _get_sort_timestamp(pair)
            return ts >= recently
        except Exception:
            return False

        
    from pprint import pprint

    def _matches_user(pair):
        pprint(pair)
        if annotator:
            if pair["previously"]:
                if pair["previously"].get("annotator") != annotator:
                    return False
            else:
                if pair["original"].get("annotator") != annotator:
                    return False

        if reviewer:
            if not pair["previously"]:
                return False
            if pair["previously"].get("reviewer") != reviewer:
                return False
        
        return True

    
    all_pairs = [p for p in all_pairs if _matches_user(p)]



    all_pairs = [p for p in all_pairs if _is_recent(p)]

    if not all_pairs:
        return {"count": 0, "items": []}

    items = all_pairs[:limit]

    return {
        "count": len(items),
        "items": items,
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8081)
